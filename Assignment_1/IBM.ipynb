{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 1 and 2 \n",
    "\n",
    "f is the source language\n",
    "\n",
    "e is the target language\n",
    "\n",
    "\n",
    "data representation: (source, target, alignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from aer import *\n",
    "import codecs\n",
    "from abc import ABC\n",
    "import pickle as cPickle\n",
    "import os\n",
    "from scipy.special import psi, gammaln\n",
    "\n",
    "\n",
    "# READING\n",
    "\n",
    "TRAINING_DIRECTORY = 'data/training/'\n",
    "TRAINING_ENGLISH_FILENAME = 'hansards.36.2.e'\n",
    "TRAINING_FRENCH_FILENAME = 'hansards.36.2.f'\n",
    "TRAINING_PAIR_FILENAME = 'training_pairs'\n",
    "\n",
    "VALIDATION_DIRECTORY = 'data/validation/'\n",
    "VALIDATION_ENGLISH_FILENAME = 'dev.e'\n",
    "VALIDATION_FRENCH_FILENAME = 'dev.f'\n",
    "VALIDATION_PAIR_FILENAME = 'validation_pairs'\n",
    "VALIDATION_ALIGNMENTS_FILENAME = 'dev.wa.nonullalign'\n",
    "\n",
    "TEST_DIRECTORY = 'data/testing/'\n",
    "TEST_ENGLISH_FILENAME = 'test/test.e'\n",
    "TEST_FRENCH_FILENAME = 'test/test.f'\n",
    "TEST_ALIGNMENTS_FILENAME = 'answers/test.wa.nonullalign'\n",
    "\n",
    "# WRITING\n",
    "\n",
    "OUTPUT_DIR = \"deliverables/\"\n",
    "\n",
    "TEST_ALIGNMENTS_OUTPUT_IBM1 = \"ibm1.mle.naacl\"\n",
    "TEST_ALIGNMENTS_OUTPUT_IBM1B = \"ibm1.vb.naacl\"\n",
    "TEST_ALIGNMENTS_OUTPUT_IBM2 = \"ibm2.mle.naacl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AER(predictions, test):\n",
    "    metric = AERSufficientStatistics()\n",
    "\n",
    "    for gold, pred in zip(test, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "\n",
    "    return metric.aer()\n",
    "\n",
    "# todo add test\n",
    "def read_data(english_file, french_file, save = 0):\n",
    "    with open(english_file) as f:\n",
    "        sentences_english = f.read().splitlines()\n",
    "    with open(french_file) as f:\n",
    "        sentences_french = f.read().splitlines()\n",
    "\n",
    "    paired = []\n",
    "    for i, sentence_english in enumerate(sentences_english):\n",
    "        paired.append([(\"null \" + sentence_english).split(\" \")[0:-1],\n",
    "                       sentences_french[i].split(\" \")[0:-1]])\n",
    "    \n",
    "    if save:\n",
    "        cPickle.dump(training_data, open(str(TRAINING_DIRECTORY + TRAINING_PAIR_FILENAME), \"wb\"))\n",
    "        cPickle.dump(validation_data, open(str(VALIDATION_DIRECTORY + VALIDATION_PAIR_FILENAME), \"wb\"))\n",
    "\n",
    "    return paired\n",
    "\n",
    "def get_validation_alignments(path = VALIDATION_DIRECTORY + VALIDATION_ALIGNMENTS_FILENAME):\n",
    "    validation_alignments = read_naacl_alignments(path)\n",
    "    return validation_alignments\n",
    "\n",
    "def get_vocabulary_size( data):\n",
    "    frenchWords = []\n",
    "    for pair in (data):\n",
    "        for word in pair[1]:\n",
    "            frenchWords.append(word)\n",
    "    return len(frenchWords)\n",
    "\n",
    "def save_as_naacl(alignments, model_name):\n",
    "    converted = \"\"\n",
    "    for i, sentence in enumerate(alignments):\n",
    "        sentence = sorted(list(sentence))\n",
    "        for alignment in sentence:\n",
    "            converted += str(i+1) + ' ' + str(alignment[0]) + ' ' + str(alignment[1]) + ' ' + 'S' + '\\n'\n",
    "\n",
    "    filepath = OUTPUT_DIR + model_name\n",
    "    \n",
    "    file = open(filepath, \"w\")\n",
    "    file.write(converted)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBM_base(ABC):\n",
    "    UNIFORM_INIT = \"uniform\"\n",
    "    RANDOM_INIT = \"random\"\n",
    "\n",
    "    init_method = None\n",
    "    model_name = None\n",
    "    t = []\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    val_alignments = []\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def get_alignments(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    \n",
    "    def calculate_log_likelihood(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate_train_epoch(self):\n",
    "        pass\n",
    "\n",
    "    def set_t(self, t):\n",
    "        self.t = t\n",
    "        \n",
    "    def empty_init(self, train_data):\n",
    "        t = {}\n",
    "        for i, pair in enumerate(train_data):\n",
    "            for english_word in pair[0]:\n",
    "                if english_word not in t:\n",
    "                    t[english_word] = {}\n",
    "                for french_word in pair[1]:\n",
    "                    t[english_word][french_word] = 0\n",
    "\n",
    "        return t\n",
    "\n",
    "    def uniform_init(self, t):\n",
    "        # Uniform init of the translation probabilities\n",
    "        new_t = {}\n",
    "\n",
    "        for key in t:\n",
    "            new_t[key] = {}\n",
    "            vocab_size = len((t[key].keys()))\n",
    "\n",
    "            for sec_key in t[key]:\n",
    "                new_t[key][sec_key] = 1.0 / vocab_size\n",
    "        \n",
    "        self.t = new_t\n",
    "\n",
    "    def random_init(self, t):\n",
    "        # Random init of the translation probabilities\n",
    "        new_t = {}\n",
    "        \n",
    "        for key in t:\n",
    "            new_t[key] = {}\n",
    "\n",
    "            for sec_key in t[key]:\n",
    "                new_t[key][sec_key] = random.random()\n",
    "\n",
    "            normalizer = sum(new_t[key].values())\n",
    "            new_t[key] = {k: v / normalizer for k, v in new_t[key].items()}\n",
    "        \n",
    "        self.t = new_t\n",
    "            \n",
    "    def init_empty_english_counts(self, t):\n",
    "        empty_counts = {}\n",
    "        english_empty_counts = {}\n",
    "\n",
    "        for key in t:\n",
    "            empty_counts[key] = {}\n",
    "            english_empty_counts[key] = 0.0\n",
    "            for sec_key in t[key]:\n",
    "                empty_counts[key][sec_key] = 0.0\n",
    "\n",
    "        return english_empty_counts, empty_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBM1(IBM_base):\n",
    "\n",
    "    def __init__(self, train_data, val_data, val_alignments, init_method=\"uniform\"):\n",
    "        self.init_method = init_method\n",
    "        self.model_name = \"IBM1\"\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.val_alignments = val_alignments\n",
    "        \n",
    "        t = self.empty_init(train_data)\n",
    "        \n",
    "        if self.init_method == self.UNIFORM_INIT:\n",
    "            self.uniform_init(t)\n",
    "        elif self.init_method == self.RANDOM_INIT:\n",
    "            self.random_init(t)\n",
    "        else:\n",
    "            print(\"Invalid init method, defaulting to uniform\")\n",
    "            self.init_method = self.UNIFORM_INIT\n",
    "            self.uniform_init(t)\n",
    "\n",
    "    def get_alignments(self, val_pairs, t):\n",
    "        \"\"\"Get the predicted alignments on sentence pairs from a trained ibm model 1 or 2\"\"\"\n",
    "        alignments = []\n",
    "        for k, val_pair in enumerate(val_pairs):\n",
    "            alignments.append(set())\n",
    "            for j, french_word in enumerate(val_pair[1]):\n",
    "                max_prob = 0.0\n",
    "                alignment = 0\n",
    "\n",
    "                for i, english_word in enumerate(val_pair[0]):\n",
    "                    if english_word in t:\n",
    "                        if french_word in t[english_word]:\n",
    "                            align_prob = t[english_word][french_word]\n",
    "\n",
    "                    if align_prob > max_prob:\n",
    "                        max_prob = align_prob\n",
    "                        alignment = i\n",
    "                if alignment is not 0:\n",
    "                    alignments[k].add((alignment, j + 1))\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def evaluate_train_epoch(self, t):\n",
    "        predictions = self.get_alignments(self.val_data, t)\n",
    "\n",
    "        aer = get_AER(predictions, self.val_alignments)\n",
    "\n",
    "        return aer\n",
    "    \n",
    "    def evaluate(self, t, data, alignments):\n",
    "        predictions = self.get_alignments(data, t)\n",
    "        \n",
    "        aer = get_AER(predictions, alignments)\n",
    "        \n",
    "        return aer\n",
    "        \n",
    "    def calculate_log_likelihood(self, t):\n",
    "        log_likelihood = 0\n",
    "        alignments  = self.get_alignments(self.train_data, t)\n",
    "        \n",
    "        for k, alignment in enumerate(alignments):\n",
    "            prob = 0\n",
    "            e = self.train_data[k][0]\n",
    "            f = self.train_data[k][1]\n",
    "            \n",
    "            for j, i in alignment:\n",
    "                prob += np.log(t[e[j]][f[i - 1]])\n",
    "            \n",
    "            log_likelihood += prob\n",
    "        \n",
    "        return log_likelihood / len(self.train_data)\n",
    "        \n",
    "    def train(self, treshold, aer_epochs_treshold = 5):\n",
    "        print(\"Started training \" + self.model_name)\n",
    "\n",
    "        log_likelihood = []\n",
    "        aers = []\n",
    "        \n",
    "        t = self.t\n",
    "        best_t = t\n",
    "\n",
    "        number_of_sentences = len(self.train_data)\n",
    "        min_aer = float('inf')\n",
    "        epoch = 0\n",
    "\n",
    "        english_empty_counts, empty_counts = self.init_empty_english_counts(t)\n",
    "        # computing empty counts\n",
    "        empty_counts = {}\n",
    "        english_empty_counts = {}\n",
    "        for key in t:\n",
    "            empty_counts[key] = {}\n",
    "            english_empty_counts[key] = 0.0\n",
    "            for secKey in t[key]:\n",
    "                empty_counts[key][secKey] = 0.0\n",
    "\n",
    "\n",
    "        converged = False\n",
    "\n",
    "        while not converged:\n",
    "            start = time.time()\n",
    "            log_like = 0\n",
    "            epoch += 1\n",
    "\n",
    "            counts = empty_counts\n",
    "            english_counts = english_empty_counts\n",
    "\n",
    "            # Expectation - step\n",
    "            for pair in self.train_data:\n",
    "\n",
    "                for j, french_word in enumerate(pair[1]):\n",
    "                    normalizer = 0.0\n",
    "                    for i, english_word in enumerate(pair[0]):\n",
    "                        normalizer += t[english_word][french_word]\n",
    "\n",
    "                    for i, english_word in enumerate(pair[0]):\n",
    "                        delta = t[english_word][french_word] / normalizer\n",
    "                        counts[english_word][french_word] += delta\n",
    "                        english_counts[english_word] += delta\n",
    "            \n",
    "            # Maximization - step \n",
    "            for english_word in t:\n",
    "                for french_word in t[english_word]:\n",
    "                    t[english_word][french_word] = counts[english_word][french_word] / english_counts[english_word]\n",
    "\n",
    "            \n",
    "            log_likelihood.append(self.calculate_log_likelihood(t))\n",
    "\n",
    "            aer = self.evaluate_train_epoch(t)\n",
    "            aers.append(aer)\n",
    "\n",
    "            if aer < min_aer:\n",
    "                min_aer = aer\n",
    "                best_t = t\n",
    "\n",
    "            if epoch > aer_epochs_treshold:\n",
    "                if len(log_likelihood) > 1:\n",
    "                    diff = log_likelihood[-1] - log_likelihood[-2]\n",
    "                    if diff < treshold:\n",
    "                        converged = True\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"epoch: \", epoch, \" aer: \", aer, \" loglikelihood: \", log_likelihood[-1], \" time: \", end - start)\n",
    "\n",
    "        self.t = t\n",
    "\n",
    "        return t, best_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBM2(IBM_base):\n",
    "    IBM1_INIT = \"ibm1_init\"\n",
    "\n",
    "    a = []\n",
    "\n",
    "    def __init__(self, train_data, val_data, val_alignments, init_method=\"uniform\", t=dict):\n",
    "        self.init_method = init_method\n",
    "        self.model_name = 'IBM2'\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.val_alignments = val_alignments\n",
    "\n",
    "        if init_method == self.UNIFORM_INIT:\n",
    "            t = self.empty_init(train_data)\n",
    "            self.uniform_init(t)\n",
    "        elif init_method == self.RANDOM_INIT:\n",
    "            t = self.empty_init(train_data)\n",
    "            self.random_init(t)\n",
    "        elif init_method == self.IBM1_INIT:\n",
    "            self.set_t(t)\n",
    "        else:\n",
    "            print(\"Invalid init method, defaulting to uniform init\")\n",
    "            self.init_method = self.UNIFORM_INIT\n",
    "            self.uniform_init(t)\n",
    "\n",
    "    def set_a(self, a):\n",
    "        self.a = a\n",
    "\n",
    "    def init_a(self):\n",
    "        ''' Initialize voger count parameter vector'''\n",
    "        a_counts = {}\n",
    "        a = {}\n",
    "\n",
    "        for train_pair in self.train_data:\n",
    "            I = len(train_pair[0])\n",
    "            J = len(train_pair[1])\n",
    "\n",
    "            for i, eng_word in enumerate(train_pair[0]):\n",
    "                for j, french_word in enumerate(train_pair[1]):\n",
    "                    a_counts[self.a_index(i, j, I, J)] = 0.0\n",
    "\n",
    "        if self.init_method == self.UNIFORM_INIT:\n",
    "            length = len(a_counts.keys())\n",
    "            for key in a_counts:\n",
    "                a[key] = 1.0 / length\n",
    "        else:\n",
    "            for key in a_counts:\n",
    "                a[key] = random.random()\n",
    "\n",
    "            normalizer = sum(a.values())\n",
    "            a = {k: v / normalizer for k, v in a.items()}\n",
    "            \n",
    "        return a, a_counts\n",
    "\n",
    "\n",
    "    def a_index(self, i, j, I, J):\n",
    "        # get a index count\n",
    "        return math.floor(i - (j + 1.0) * I / J)\n",
    "\n",
    "\n",
    "    def get_alignments(self, val_pairs, t, a=dict):\n",
    "        \"\"\"Get the predicted alignments on sentence pairs from a trained ibm model 1 or 2\"\"\"\n",
    "        alignments = []\n",
    "        for k, val_pair in enumerate(val_pairs):\n",
    "            alignments.append(set())\n",
    "            I = len(val_pair[0])\n",
    "            J = len(val_pair[1])\n",
    "\n",
    "            for j, french_word in enumerate(val_pair[1]):\n",
    "                max_prob = 0.0\n",
    "                align_prob = float('-inf')\n",
    "                alignment = 0\n",
    "\n",
    "                for i, english_word in enumerate(val_pair[0]):\n",
    "                    if english_word in t:\n",
    "                        if french_word in t[english_word]:\n",
    "                            align_prob = t[english_word][french_word] * a[self.a_index(i, j, I, J)]\n",
    "\n",
    "                    if align_prob > max_prob:\n",
    "                        max_prob = align_prob\n",
    "                        alignment = i\n",
    "                if alignment is not 0:\n",
    "                    alignments[k].add((alignment, j + 1))\n",
    "\n",
    "        return alignments\n",
    "    \n",
    "    def calculate_log_likelihood(self, t, a):\n",
    "        log_likelihood = 0\n",
    "        alignments  = self.get_alignments(self.train_data, t, a)\n",
    "        \n",
    "        for k, alignment in enumerate(alignments):\n",
    "            prob = 0\n",
    "            e = self.train_data[k][0]\n",
    "            f = self.train_data[k][1]\n",
    "            I = len(e)\n",
    "            J = len(f)\n",
    "            \n",
    "            for i,j in alignment:\n",
    "                a_index = self.a_index(i ,j - 1,I,J)\n",
    "                a_value = a[a_index]\n",
    "                prob += np.log(t[e[i]][f[j - 1]] * a_value)\n",
    "            \n",
    "            log_likelihood += prob\n",
    "        \n",
    "        return log_likelihood / len(self.train_data)\n",
    "\n",
    "    def evaluate_train_epoch(self, t, a):\n",
    "        predictions = self.get_alignments(self.val_data, t, a)\n",
    "\n",
    "        aer = get_AER(predictions, self.val_alignments)\n",
    "\n",
    "        return aer\n",
    "    \n",
    "    def evaluate(self, t, a, data, alignments):\n",
    "        predictions = self.get_alignments(data, t, a)\n",
    "        \n",
    "        aer = get_AER(predictions, alignments)\n",
    "        \n",
    "        print(\"Test AER \" + self.model_name + \"-\" + self.init_method + \": \" + str(aer))\n",
    "        print(\"\")\n",
    "        \n",
    "        return aer\n",
    "\n",
    "    def train(self, treshold, aer_epochs_treshold = 5):\n",
    "        print(\"Started training \" + self.model_name + \"-\" + self.init_method)\n",
    "    \n",
    "        log_likelihood = []\n",
    "        aers = []\n",
    "\n",
    "        t = self.t\n",
    "        best_t = self.t\n",
    "        a, a_counts = self.init_a()\n",
    "        best_a = a\n",
    "\n",
    "        number_of_sentences = len(self.train_data)\n",
    "        min_aer = float('inf')\n",
    "        epoch = 0\n",
    "\n",
    "        english_empty_counts, empty_counts = self.init_empty_english_counts(t)\n",
    "\n",
    "        converged = False\n",
    "\n",
    "        while not converged:\n",
    "            start = time.time()\n",
    "            log_like = 0\n",
    "            epoch += 1\n",
    "\n",
    "            counts = empty_counts\n",
    "            english_counts = english_empty_counts\n",
    "\n",
    "            # Expectation - step\n",
    "            for train_pair in self.train_data:\n",
    "                I = len(train_pair[0])\n",
    "                J = len(train_pair[1])\n",
    "\n",
    "\n",
    "                for j, french_word in enumerate(train_pair[1]):\n",
    "                    normalizer = 0.0\n",
    "\n",
    "                    for i, english_word in enumerate(train_pair[0]):\n",
    "                        a_index = self.a_index(i, j, I, J)\n",
    "                        normalizer += t[english_word][french_word] * a[a_index]\n",
    "\n",
    "                    for i, english_word in enumerate(train_pair[0]):\n",
    "                        a_index = self.a_index(i, j, I, J)\n",
    "                        delta = a[a_index] * t[english_word][french_word] / normalizer\n",
    "\n",
    "                        a_counts[a_index] += delta\n",
    "                        counts[english_word][french_word] += delta\n",
    "                        english_counts[english_word] += delta\n",
    "\n",
    "            # Maximization - step\n",
    "            for english_key in t:\n",
    "                for french_key in t[english_key]:\n",
    "                    t[english_key][french_key] = counts[english_key][french_key] / english_counts[english_key]\n",
    "\n",
    "            normalizer = sum(a_counts.values())\n",
    "            a = {k: v / normalizer for k, v in a_counts.items()}\n",
    "\n",
    "            log_likelihood.append(self.calculate_log_likelihood(t,a))\n",
    "\n",
    "            aer = self.evaluate_train_epoch(t, a)\n",
    "            aers.append(aer)\n",
    "\n",
    "            if aer < min_aer:\n",
    "                min_aer = aer\n",
    "                best_a = a\n",
    "                best_t = t\n",
    "\n",
    "            if epoch > aer_epochs_treshold:\n",
    "                if len(log_likelihood) > 1:\n",
    "                    diff = log_likelihood[-1] - log_likelihood[-2]\n",
    "                    if diff < treshold:\n",
    "                        converged = True\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"epoch: \", epoch, \" aer: \", aer, \" loglikelihood: \", log_likelihood[-1], \" time: \", end - start)\n",
    "        \n",
    "        self.t = t\n",
    "        self.a = a\n",
    "        \n",
    "        return t, a, best_t, best_a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Variational Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBM1_bayesian(IBM_base):\n",
    "    BAYESIAN_INIT = \"bayesian\"\n",
    "\n",
    "    u = []\n",
    "    alpha = 0\n",
    "    french_words = 0\n",
    "\n",
    "    def __init__(self, train_data, val_data, val_alignments, french_words = 0, alpha=0.0005, init_method=\"bayesian\"):\n",
    "        self.init_method = init_method\n",
    "        self.model_name = 'IBM1_bayesian'\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.val_alignments = val_alignments\n",
    "        self.alpha = alpha\n",
    "        self.french_words = french_words\n",
    "\n",
    "        t = self.empty_init(train_data)\n",
    "\n",
    "        if self.init_method == self.BAYESIAN_INIT:\n",
    "            self.bayes_init(t)\n",
    "\n",
    "\n",
    "    def bayes_init(self,t):\n",
    "        trans = {}\n",
    "        unseen = {}\n",
    "\n",
    "        for key in t:\n",
    "            trans[key] = {}\n",
    "            unseen[key] = 1.0 / self.french_words\n",
    "\n",
    "            for sec_key in t[key]:\n",
    "                trans[key][sec_key] = 1.0 / self.french_words\n",
    "\n",
    "        self.t = trans\n",
    "        self.u = unseen\n",
    "\n",
    "    def bayesian_maximization(self, counts, normalizer):\n",
    "        return psi(counts + self.alpha) - psi(normalizer + self.alpha * self.french_words)\n",
    "\n",
    "    def evaluate_train_epoch(self, t, u):\n",
    "        predictions = self.get_alignments(self.val_data, t, u)\n",
    "\n",
    "        aer = get_AER(predictions, self.val_alignments)\n",
    "\n",
    "        return aer\n",
    "    \n",
    "    def evaluate(self, t, u, data, alignments):\n",
    "        predictions = self.get_alignments(data, t, u)\n",
    "        \n",
    "        aer = get_AER(predictions, alignments)\n",
    "        \n",
    "        print(\"Test AER \" + self.model_name + \"-\" + str(self.alpha) + \": \" + str(aer))\n",
    "        print(\"\")\n",
    "        \n",
    "        return aer\n",
    "\n",
    "    def train(self, treshold, aer_epochs_treshold = 5):\n",
    "        print(\"Started training \" + self.model_name + \"-\" + str(self.alpha))\n",
    "\n",
    "        log_likelihood = []\n",
    "        aers = []\n",
    "        min_aer = float('inf')\n",
    "        epoch = 0\n",
    "\n",
    "        t = self.t\n",
    "        u = self.u\n",
    "        best_t = self.t\n",
    "        best_u = self.u\n",
    "\n",
    "        number_of_sentences = len(self.train_data)\n",
    "\n",
    "        converged = False\n",
    "\n",
    "        english_empty_counts, empty_counts = self.init_empty_english_counts(t)\n",
    "\n",
    "        while not converged:\n",
    "            start = time.time()\n",
    "            epoch += 1\n",
    "            log_like = 0\n",
    "\n",
    "            counts = empty_counts\n",
    "            english_counts = english_empty_counts\n",
    "\n",
    "            for train_pair in self.train_data:\n",
    "                for j, french_word in enumerate(train_pair[1]):\n",
    "                    normalizer = 0.0\n",
    "\n",
    "                    for i, english_word in enumerate(train_pair[0]):\n",
    "                        normalizer += t[english_word][french_word]\n",
    "\n",
    "                    log_like += np.log(normalizer)\n",
    "\n",
    "                    for i, english_word in enumerate(train_pair[0]):\n",
    "                        delta = t[english_word][french_word] / normalizer\n",
    "                        counts[english_word][french_word] += delta\n",
    "                        english_counts[english_word] += delta\n",
    "\n",
    "            for english_word in t:\n",
    "                u[english_word] = self.bayesian_maximization(0, english_counts[english_word])\n",
    "                for french_word in t[english_word]:\n",
    "                    t[english_word][french_word] = self.bayesian_maximization(counts[english_word][french_word], english_counts[english_word])\n",
    "            \n",
    "            log_likelihood.append(self.calculate_log_likelihood(log_like, t, counts))\n",
    "\n",
    "            aer = self.evaluate_train_epoch(t, u)\n",
    "            aers.append(aer)\n",
    "\n",
    "            if aer < min_aer:\n",
    "                min_aer = aer\n",
    "                best_u = u\n",
    "                best_t = t\n",
    "\n",
    "            if epoch > aer_epochs_treshold:\n",
    "                if len(log_likelihood) > 1:\n",
    "                    diff = log_likelihood[-1] - log_likelihood[-2]\n",
    "                    if diff < treshold:\n",
    "                        converged = True\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"epoch: \", epoch, \" aer: \", aer, \" loglikelihood: \", log_likelihood[-1], \" time: \", end - start)\n",
    "        \n",
    "        self.t = t\n",
    "        self.u = u\n",
    "        \n",
    "        return t, u, best_t, best_u\n",
    "\n",
    "    def get_alignments(self, val_pairs, t, u=dict):\n",
    "        \"\"\"Get the predicted alignments on sentence pairs from a trained ibm model 1 or 2\"\"\"\n",
    "        alignments = []\n",
    "        for k, val_pair in enumerate(val_pairs):\n",
    "            alignments.append(set())\n",
    "            I = len(val_pair[0])\n",
    "            J = len(val_pair[1])\n",
    "\n",
    "            for j, french_word in enumerate(val_pair[1]):\n",
    "                max_prob = 0.0\n",
    "                align_prob = float('-inf')\n",
    "                alignment = 0\n",
    "\n",
    "                for i, english_word in enumerate(val_pair[0]):\n",
    "                    if english_word in t:\n",
    "                        if french_word in t[english_word]:\n",
    "                            align_prob = t[english_word][french_word]\n",
    "                        else:\n",
    "                            align_prob = u[english_word]\n",
    "                    if align_prob > max_prob:\n",
    "                        max_prob = align_prob\n",
    "                        alignment = i\n",
    "                if alignment is not 0:\n",
    "                    alignments[k].add((alignment, j + 1))\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def calculate_log_likelihood(self, log_like, t, counts):\n",
    "        alpha = self.alpha\n",
    "        gamma_alpha = gammaln(alpha)\n",
    "\n",
    "        for english_word, english_probs in t.items():\n",
    "            lamb = 0\n",
    "\n",
    "            for french_word, french_probs in english_probs.items():\n",
    "                t[english_word][french_word] = np.exp(french_probs)\n",
    "                count = counts[english_word][french_word]\n",
    "\n",
    "                log_like += (french_probs * (-count) + gammaln(alpha + count) - gamma_alpha)\n",
    "                lamb += count\n",
    "\n",
    "            lamb += self.french_words * alpha\n",
    "            log_like += gammaln(alpha * self.french_words) - gammaln(lamb)\n",
    "\n",
    "        return log_like / len(self.train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Models experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and define global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AER_EPOCHS_TRESHOLD = 5\n",
    "\n",
    "train_pairs = read_data(TRAINING_DIRECTORY + TRAINING_ENGLISH_FILENAME, TRAINING_DIRECTORY + TRAINING_FRENCH_FILENAME)\n",
    "val_pairs = read_data(VALIDATION_DIRECTORY + VALIDATION_ENGLISH_FILENAME, VALIDATION_DIRECTORY + VALIDATION_FRENCH_FILENAME)\n",
    "test_pairs = read_data(TEST_DIRECTORY + TEST_ENGLISH_FILENAME, TEST_DIRECTORY + TEST_FRENCH_FILENAME)\n",
    "\n",
    "val_alignments = get_validation_alignments()\n",
    "test_alignments = get_validation_alignments(TEST_DIRECTORY + TEST_ALIGNMENTS_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM1 TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training IBM1\n",
      "epoch:  1  aer:  0.37238095238095237  loglikelihood:  -70.10828042092986  time:  190.25556135177612\n",
      "epoch:  2  aer:  0.3262955854126679  loglikelihood:  -55.27673877324344  time:  183.75935697555542\n",
      "epoch:  3  aer:  0.31892411143131605  loglikelihood:  -49.65922203069056  time:  183.7144718170166\n",
      "epoch:  4  aer:  0.3150816522574448  loglikelihood:  -46.65037438364726  time:  185.4859380722046\n",
      "epoch:  5  aer:  0.3112391930835735  loglikelihood:  -44.79100360891826  time:  183.15603828430176\n",
      "epoch:  6  aer:  0.3089509143407122  loglikelihood:  -43.48387748603292  time:  177.70188283920288\n",
      "epoch:  7  aer:  0.3108758421559191  loglikelihood:  -42.50796801564615  time:  193.95269203186035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2966276668960771"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRESHOLD = 1\n",
    "\n",
    "ibm1 = IBM1(train_pairs, val_pairs, val_alignments)\n",
    "t_ibm1, best_t_ibm1 = ibm1.train(TRESHOLD, AER_EPOCHS_TRESHOLD)\n",
    "\n",
    "ibm1.evaluate(best_t_ibm1, test_pairs, test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM2 TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training IBM2-ibm1_init\n",
      "epoch:  1  aer:  0.35838150289017345  loglikelihood:  -101.77492484493682  time:  385.15889501571655\n",
      "epoch:  2  aer:  0.31160115052732507  loglikelihood:  -94.89890847527597  time:  358.75532817840576\n",
      "epoch:  3  aer:  0.2545454545454545  loglikelihood:  -91.038803991066  time:  346.1818251609802\n",
      "epoch:  4  aer:  0.25982742090124644  loglikelihood:  -88.47934333418031  time:  341.8149130344391\n",
      "epoch:  5  aer:  0.2569511025886865  loglikelihood:  -86.61207143846023  time:  340.77851390838623\n",
      "epoch:  6  aer:  0.25047984644913623  loglikelihood:  -85.26053332121208  time:  344.48517203330994\n",
      "Test AER IBM2-ibm1_init: 0.22195395360869186\n",
      "\n",
      "Started training IBM2-uniform\n",
      "epoch:  1  aer:  0.2857142857142857  loglikelihood:  -133.66222045813393  time:  374.9105188846588\n",
      "epoch:  2  aer:  0.24282982791586993  loglikelihood:  -107.83203042557234  time:  370.7394459247589\n",
      "epoch:  3  aer:  0.23823246878001925  loglikelihood:  -97.63920258332907  time:  371.2373161315918\n",
      "epoch:  4  aer:  0.2336538461538461  loglikelihood:  -92.61763210678565  time:  357.9017479419708\n",
      "epoch:  5  aer:  0.23173076923076918  loglikelihood:  -89.55901670904626  time:  358.95485186576843\n",
      "epoch:  6  aer:  0.2288461538461538  loglikelihood:  -87.51451239126011  time:  348.02730894088745\n",
      "epoch:  7  aer:  0.2283236994219653  loglikelihood:  -86.00957334626911  time:  349.0627188682556\n",
      "Test AER IBM2-uniform: 0.20706065587955347\n",
      "\n",
      "Started training IBM2-random\n",
      "epoch:  1  aer:  0.3783269961977186  loglikelihood:  -126.74631501813671  time:  344.9774127006531\n",
      "epoch:  2  aer:  0.30563514804202485  loglikelihood:  -104.89695200577641  time:  347.34887313842773\n",
      "epoch:  3  aer:  0.28489483747609945  loglikelihood:  -96.21191908160165  time:  347.00335597991943\n",
      "epoch:  4  aer:  0.2705544933078394  loglikelihood:  -91.75952619411547  time:  337.6541180610657\n",
      "epoch:  5  aer:  0.2540747842761265  loglikelihood:  -88.92842220120835  time:  341.6165351867676\n",
      "epoch:  6  aer:  0.2514395393474088  loglikelihood:  -87.04776029533234  time:  2053.9448812007904\n",
      "Test AER IBM2-random: 0.23230928546772522\n",
      "\n",
      "Started training IBM2-random\n",
      "epoch:  1  aer:  0.5331412103746398  loglikelihood:  -134.1697110721556  time:  391.1880371570587\n",
      "epoch:  2  aer:  0.38306063522617906  loglikelihood:  -114.146384773439  time:  379.85770177841187\n",
      "epoch:  3  aer:  0.30000000000000004  loglikelihood:  -104.65914775477684  time:  360.63057494163513\n",
      "epoch:  4  aer:  0.25769230769230766  loglikelihood:  -98.93494230374495  time:  369.4970769882202\n",
      "epoch:  5  aer:  0.23942307692307696  loglikelihood:  -95.1331512419545  time:  375.446494102478\n",
      "epoch:  6  aer:  0.23246878001921234  loglikelihood:  -92.45099858125803  time:  381.397971868515\n",
      "epoch:  7  aer:  0.23032629558541262  loglikelihood:  -90.48560410704177  time:  2327.6225321292877\n",
      "Test AER IBM2-random: 0.22070515036294502\n",
      "\n",
      "Started training IBM2-random\n",
      "epoch:  1  aer:  0.3776290630975143  loglikelihood:  -123.96419640759666  time:  622.178927898407\n",
      "epoch:  2  aer:  0.2777777777777778  loglikelihood:  -102.6009914102758  time:  423.45242619514465\n",
      "epoch:  3  aer:  0.2603266090297791  loglikelihood:  -94.31674161650223  time:  367.9050261974335\n",
      "epoch:  4  aer:  0.2548076923076923  loglikelihood:  -90.12865669931335  time:  368.1151850223541\n",
      "epoch:  5  aer:  0.24975984630163306  loglikelihood:  -87.63726994448656  time:  348.3709399700165\n",
      "epoch:  6  aer:  0.24446583253128007  loglikelihood:  -85.90364661221444  time:  350.9880611896515\n",
      "Test AER IBM2-random: 0.22635310392529828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 2\n",
    "init_methods = [\"ibm1_init\", \"uniform\", \"random1\", \"random2\", \"random3\"]\n",
    "results = {\"uniform\" : {}, \"random1\": {}, \"random2\": {}, \"random3\": {}, \"ibm1_init\": {}}\n",
    "\n",
    "\n",
    "for init_method in init_methods:\n",
    "    if \"random\" in init_method:\n",
    "        random_iteration = init_method[-1]\n",
    "        init_method = \"random\"\n",
    "    else:\n",
    "        random_iteration = \"\"\n",
    "        \n",
    "    ibm2 = IBM2(train_pairs, val_pairs, val_alignments, init_method, best_t_ibm1)\n",
    "    t, a, best_t, best_a = ibm2.train(TRESHOLD, AER_EPOCHS_TRESHOLD)\n",
    "    \n",
    "    test_aer = ibm2.evaluate(best_t, best_a, test_pairs, test_alignments)\n",
    "\n",
    "    results[init_method + random_iteration]['a'] = a\n",
    "    results[init_method + random_iteration]['t'] = t\n",
    "    results[init_method + random_iteration]['best_t'] = best_t\n",
    "    results[init_method + random_iteration]['best_a'] = best_a\n",
    "    results[init_method + random_iteration]['test_aer'] = test_aer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Bayesian TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training IBM1_bayesian-0.0005\n",
      "epoch:  1  aer:  0.37845567206863684  loglikelihood:  -440.2746923840363  time:  179.78928422927856\n",
      "epoch:  2  aer:  0.3621517771373679  loglikelihood:  -265.80112017365946  time:  248.16629004478455\n",
      "epoch:  3  aer:  0.35796545105566224  loglikelihood:  -255.21125918693096  time:  244.26867389678955\n",
      "epoch:  4  aer:  0.35700575815738966  loglikelihood:  -254.06164601621165  time:  249.41818284988403\n",
      "epoch:  5  aer:  0.3541266794625719  loglikelihood:  -255.0378803136446  time:  252.44468903541565\n",
      "epoch:  6  aer:  0.35158501440922185  loglikelihood:  -256.73724036676913  time:  229.5512731075287\n",
      "Test AER IBM1_bayesian-0.0005: 0.32210254200775523\n",
      "\n",
      "Started training IBM1_bayesian-0.005\n",
      "epoch:  1  aer:  0.41522157996146436  loglikelihood:  -336.80185148632097  time:  178.47593903541565\n",
      "epoch:  2  aer:  0.39633558341369335  loglikelihood:  -177.51317332334577  time:  308.02882194519043\n",
      "epoch:  3  aer:  0.3876567020250723  loglikelihood:  -171.79737032030852  time:  2691.000206232071\n",
      "epoch:  4  aer:  0.38201160541586077  loglikelihood:  -172.01653034582068  time:  230.7788450717926\n",
      "epoch:  5  aer:  0.3810444874274661  loglikelihood:  -174.5058985187912  time:  227.6072061061859\n",
      "epoch:  6  aer:  0.3719806763285024  loglikelihood:  -177.97913358978866  time:  238.20900893211365\n",
      "Test AER IBM1_bayesian-0.005: 0.35011282763409135\n",
      "\n",
      "Started training IBM1_bayesian-0.05\n",
      "epoch:  1  aer:  0.6953781512605042  loglikelihood:  -274.9802339485202  time:  175.7135238647461\n",
      "epoch:  2  aer:  0.5586136595310907  loglikelihood:  -141.1951308252403  time:  257.39321780204773\n",
      "epoch:  3  aer:  0.5161616161616162  loglikelihood:  -135.99279651909188  time:  240.3159008026123\n",
      "epoch:  4  aer:  0.4818548387096774  loglikelihood:  -138.54932229539895  time:  241.19241094589233\n",
      "epoch:  5  aer:  0.47336683417085423  loglikelihood:  -142.6582431797054  time:  252.5298628807068\n",
      "epoch:  6  aer:  0.46215943491422806  loglikelihood:  -147.55031896837897  time:  253.55834007263184\n",
      "Test AER IBM1_bayesian-0.05: 0.47065802683216207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 1\n",
    "french_words = get_vocabulary_size(train_pairs)\n",
    "results = {}\n",
    "\n",
    "for alpha in [0.0005, 0.005, 0.05]:\n",
    "    results[alpha] = {}\n",
    "    \n",
    "    ibm1_bayesian = IBM1_bayesian(train_pairs, val_pairs, val_alignments, french_words, alpha)\n",
    "    t_bayesian, u_bayesian, best_t_bayesian, best_u_bayesian = ibm1_bayesian.train(TRESHOLD, AER_EPOCHS_TRESHOLD)\n",
    "    test_aer = ibm1_bayesian.evaluate(best_t_bayesian, best_u_bayesian, test_pairs, test_alignments)\n",
    "    \n",
    "    results[alpha][\"t\"] = t_bayesian\n",
    "    results[alpha][\"u\"] = u_bayesian\n",
    "    results[alpha][\"best_t_bayesian\"] = best_t_bayesian\n",
    "    results[alpha][\"best_u_bayesian\"] = best_u_bayesian\n",
    "    results[alpha][\"test_aer\"] = test_aer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train best models + NAACL generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = ibm1.get_alignments(test_pairs, best_t_ibm1)\n",
    "save_as_naacl(alignments, TEST_ALIGNMENTS_OUTPUT_IBM1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training IBM2-uniform\n",
      "epoch:  1  aer:  0.2857142857142857  loglikelihood:  -133.66222045813393  time:  386.3716850280762\n",
      "epoch:  2  aer:  0.24282982791586993  loglikelihood:  -107.83203042557234  time:  356.21409368515015\n",
      "epoch:  3  aer:  0.23823246878001925  loglikelihood:  -97.63920258332907  time:  370.06926012039185\n",
      "epoch:  4  aer:  0.2336538461538461  loglikelihood:  -92.61763210678565  time:  391.10724210739136\n",
      "epoch:  5  aer:  0.23173076923076918  loglikelihood:  -89.55901670904626  time:  377.58235716819763\n",
      "epoch:  6  aer:  0.2288461538461538  loglikelihood:  -87.51451239126011  time:  371.52227568626404\n",
      "epoch:  7  aer:  0.2283236994219653  loglikelihood:  -86.00957334626911  time:  361.2159032821655\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 2\n",
    "\n",
    "ibm2 = IBM2(train_pairs, val_pairs, val_alignments, \"uniform\")\n",
    "t, a, best_t, best_a = ibm2.train(TRESHOLD, AER_EPOCHS_TRESHOLD)\n",
    "\n",
    "alignments = ibm2.get_alignments(test_pairs, best_t, best_a)\n",
    "save_as_naacl(alignments, TEST_ALIGNMENTS_OUTPUT_IBM2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM1 Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training IBM1_bayesian-0.0005\n",
      "epoch:  1  aer:  0.37845567206863684  loglikelihood:  -440.2746923840363  time:  180.38955330848694\n",
      "epoch:  2  aer:  0.3621517771373679  loglikelihood:  -265.80112017365946  time:  230.85887098312378\n",
      "epoch:  3  aer:  0.35796545105566224  loglikelihood:  -255.21125918693096  time:  247.99844408035278\n",
      "epoch:  4  aer:  0.35700575815738966  loglikelihood:  -254.06164601621165  time:  272.77419424057007\n",
      "epoch:  5  aer:  0.3541266794625719  loglikelihood:  -255.0378803136446  time:  252.38278603553772\n",
      "epoch:  6  aer:  0.35158501440922185  loglikelihood:  -256.73724036676913  time:  247.5081343650818\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 1\n",
    "french_words = get_vocabulary_size(train_pairs)\n",
    "\n",
    "ibm1_bayesian = IBM1_bayesian(train_pairs, val_pairs, val_alignments, french_words, 0.0005)\n",
    "t_bayesian, u_bayesian, best_t_bayesian, best_u_bayesian = ibm1_bayesian.train(TRESHOLD, AER_EPOCHS_TRESHOLD)\n",
    "\n",
    "alignments = ibm1_bayesian.get_alignments(test_pairs, best_t_bayesian, best_u_bayesian)\n",
    "save_as_naacl(alignments, TEST_ALIGNMENTS_OUTPUT_IBM1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM1 LL, AER TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aer_1_log = ibm1.evaluate(t, test_pairs, test_alignments)\n",
    "test_aer_1_aer = ibm1.evaluate(best_t_ibm1, test_pairs, test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM2 LL, AER TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AER IBM2-uniform: 0.20706065587955347\n",
      "\n",
      "Test AER IBM2-uniform: 0.20706065587955347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_aer_2_log = ibm2.evaluate(t, a, test_pairs, test_alignments)\n",
    "test_aer_2_aer = ibm2.evaluate(best_t, best_a, test_pairs, test_alignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM 1 B, AER TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AER IBM1_bayesian-0.0005: 0.32210254200775523\n",
      "\n",
      "Test AER IBM1_bayesian-0.0005: 0.32210254200775523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_aer_b_ll = ibm1_bayesian.evaluate(t_bayesian, u_bayesian, test_pairs, test_alignments)\n",
    "test_aer_b_aer = ibm1_bayesian.evaluate(best_t_bayesian, best_u_bayesian, test_pairs, test_alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
