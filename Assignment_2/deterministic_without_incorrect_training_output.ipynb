{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deterministic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH7nt6PsB_jS",
        "colab_type": "text"
      },
      "source": [
        "Imports needed to run the cells below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6LifADHAtJa",
        "colab_type": "code",
        "outputId": "c0d26469-4152-439d-a4ee-80bbf925b507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from functools import reduce\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive.mount('/content/gdrive')\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.3)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OAWADu_CLJ2",
        "colab_type": "text"
      },
      "source": [
        "The deterministic model implemented for the assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TaxuOgvAwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicLSTM(nn.Module):\n",
        "    def __init__(self, vocab, nb_layers=1, nb_lstm_units=100, embedding_dim=10, batch_size=64, on_gpu=False, pad_token=\"<PAD>\", unk_token=\"<UNK>\", reduced_vocab=False, dropout=0):\n",
        "        super(DeterministicLSTM, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.nb_layers = nb_layers\n",
        "        self.nb_lstm_units = nb_lstm_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.on_gpu = on_gpu\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.reduced_vocab = reduced_vocab\n",
        "        self.len_vocab = len(self.vocab) \n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.__build_model()\n",
        "        self.init_weights()\n",
        "\n",
        "    def __build_model(self):        \n",
        "        # encoder to word embeddings\n",
        "        self.encoder = nn.Embedding(\n",
        "            num_embeddings=self.len_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            padding_idx=self.vocab[self.pad_token]\n",
        "        )\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.nb_lstm_units,\n",
        "            num_layers=self.nb_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # decoder to output space\n",
        "        self.decoder = nn.Linear(self.nb_lstm_units, self.len_vocab)\n",
        "        \n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "        \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden_a = torch.zeros(self.nb_layers,\n",
        "                               self.batch_size, self.nb_lstm_units)\n",
        "        hidden_b = torch.zeros(self.nb_layers,\n",
        "                               self.batch_size, self.nb_lstm_units)\n",
        "\n",
        "        if self.on_gpu:\n",
        "            hidden_a = hidden_a.cuda()\n",
        "            hidden_b = hidden_b.cuda()\n",
        "\n",
        "        hidden_a = Variable(hidden_a)\n",
        "        hidden_b = Variable(hidden_b)\n",
        "\n",
        "        return (hidden_a, hidden_b)\n",
        "      \n",
        "    \n",
        "    def step(self, x, hidden):\n",
        "        x = self.encoder(x)\n",
        "        x = self.dropout_layer(x)\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = self.dropout_layer(x)\n",
        "        x = self.decoder(x)\n",
        "        \n",
        "        return x, hidden\n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden = self.init_hidden()\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(x.size(1)):\n",
        "            previous_x = x[:, t].unsqueeze(-1)\n",
        "            y_hat, hidden = self.step(previous_x, hidden)\n",
        "             \n",
        "            outputs.append(y_hat)\n",
        "            \n",
        "        return torch.cat(outputs, dim=1)\n",
        "        \n",
        "    \n",
        "    def loss(self, Y_hat, Y, batch_size, seq_len, vocab_size, eval=False):\n",
        "        Y_hat = Y_hat.permute(0,2,1)\n",
        "        \n",
        "        loss = F.cross_entropy(\n",
        "            Y_hat,\n",
        "            Y, \n",
        "            ignore_index=self.vocab[self.pad_token], \n",
        "            reduction=\"none\"\n",
        "        )\n",
        "        \n",
        "        loss = loss.sum()\n",
        "        \n",
        "        \n",
        "        if eval:\n",
        "            loss = loss.sum()\n",
        "        else:\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX4KAZALEVgZ",
        "colab_type": "text"
      },
      "source": [
        "Functions implemented to load and manipulate data, as well as wrapper functions for operations such as training, evaluating, and other things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_9jr_NITBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    global ONLY_ALLOW_FREQUENT_WORDS\n",
        "    global UNKNOWN\n",
        "    global PAD\n",
        "    global SOS\n",
        "    global EOS\n",
        "    global DROP_LONG_SENTENCES\n",
        "    global USE_HALF_SETS\n",
        "    \n",
        "    #file_ids are in the order train, valid, test\n",
        "    #file_ids = [\"1jGgW9oyEWuKgMt32_s9BnSJt3A5CR7Z8\", \"1zwVW6-HA3KxyDuJIXuTK1OvKeoXiPHqb\", \"1f-rz6KNWUPO5ToHeLewlHezDNUQdNnQg\"]\n",
        "    file_ids = [\"1tCivrO7xa9PzroVUw8s92nI7LtW6TOB5\", \"1zwVW6-HA3KxyDuJIXuTK1OvKeoXiPHqb\"]\n",
        "    \n",
        "    data = []\n",
        "    vocab = [PAD, UNKNOWN, SOS, EOS]\n",
        "    word_frequencies = {}\n",
        "\n",
        "\n",
        "    for file_id in file_ids:\n",
        "        sentences = json.loads(drive.CreateFile({'id': file_id}).GetContentString())\n",
        "        \n",
        "        if USE_HALF_SETS:\n",
        "            sentences = sentences[:len(sentences)//2]\n",
        "\n",
        "        if DROP_LONG_SENTENCES:\n",
        "            sentences = list(filter(lambda s: len(s) < 50, sentences))\n",
        "            \n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                if word not in word_frequencies:\n",
        "                    word_frequencies[word] = 0\n",
        "\n",
        "                word_frequencies[word] += 1\n",
        "\n",
        "        data.append(sentences)\n",
        "        \n",
        "    if not ONLY_ALLOW_FREQUENT_WORDS:\n",
        "        vocab += word_frequencies.keys()\n",
        "    \n",
        "    else:\n",
        "        updated_data = []\n",
        "        \n",
        "        for dataset in data:\n",
        "            new_sentences = []\n",
        "\n",
        "            for sentence in dataset:\n",
        "                new_sentences.append([w if word_frequencies[w] > 1 else UNKNOWN for w in sentence])\n",
        "\n",
        "            updated_data.append(new_sentences)\n",
        "        \n",
        "        data = updated_data\n",
        "        \n",
        "        vocab += list(filter(lambda w: word_frequencies[w] > 1, word_frequencies.keys()))\n",
        "    \n",
        "    dict_vocab = {}\n",
        "    \n",
        "    for i in range(len(vocab)):\n",
        "        dict_vocab[vocab[i]] = i\n",
        "    \n",
        "    data.append(dict_vocab)\n",
        "    data.append(vocab)\n",
        "\n",
        "    return data\n",
        "\n",
        "          \n",
        "def get_indexed_vocab(vocab):\n",
        "    indexed_vocab = {}\n",
        "    counter = 0\n",
        "\n",
        "    for w in vocab:\n",
        "        indexed_vocab[w] = counter\n",
        "        counter += 1\n",
        "\n",
        "    return indexed_vocab\n",
        "\n",
        "\n",
        "def get_longest_sentence(*datasets):\n",
        "    longest_sentence = 0\n",
        "\n",
        "    for ds in datasets:\n",
        "        candidate = len(max(ds, key=len))\n",
        "\n",
        "        if candidate > longest_sentence:\n",
        "            longest_sentence = candidate\n",
        "\n",
        "    return longest_sentence\n",
        "\n",
        "\n",
        "def get_minibatches(dataset):\n",
        "    global MINIBATCH_SIZE\n",
        "    len_d = len(dataset)\n",
        "\n",
        "    cutoff = len_d % MINIBATCH_SIZE\n",
        "    cut_dataset = dataset[: len_d - cutoff]\n",
        "\n",
        "    return [cut_dataset[i * MINIBATCH_SIZE: (i + 1) * MINIBATCH_SIZE] for i in range(len_d//MINIBATCH_SIZE)]\n",
        "\n",
        "\n",
        "def format_minibatches(minibatches, vocab):\n",
        "    global PAD\n",
        "    global SOS\n",
        "    global EOS\n",
        "    global DEVICE\n",
        "\n",
        "    pad_token = vocab[PAD]\n",
        "    formatted_minibatches = []\n",
        "\n",
        "    for minibatch in minibatches:\n",
        "        x = []\n",
        "        y = []\n",
        "        x_lengths = []\n",
        "        longest_sentence = len(max(minibatch, key=len))\n",
        "\n",
        "        for sentence in minibatch:\n",
        "            s_len = len(sentence)\n",
        "            x_lengths.append(s_len)\n",
        "            v_s = [vocab[w] for w in sentence]\n",
        "\n",
        "            ind_s_x = [vocab[SOS]] + v_s\n",
        "            pad_s_x = np.ones((longest_sentence)) * pad_token\n",
        "            pad_s_x[0:s_len] = np.array(ind_s_x[:s_len])\n",
        "\n",
        "            x.append(pad_s_x)\n",
        "\n",
        "            ind_s_y = v_s + [vocab[SOS]]\n",
        "            pad_s_y = np.ones((longest_sentence)) * pad_token\n",
        "            pad_s_y[0:s_len] = np.array(ind_s_y[:s_len])\n",
        "\n",
        "            y.append(pad_s_y)\n",
        "\n",
        "        # have to sort for later use by padded_sequence\n",
        "        sorted_indexes = sorted(range(len(x_lengths)),\n",
        "                                key=x_lengths.__getitem__)\n",
        "        sorted_indexes.reverse()\n",
        "\n",
        "        x = torch.LongTensor([x[i] for i in sorted_indexes]).to(DEVICE)\n",
        "        y = torch.LongTensor([y[i] for i in sorted_indexes]).to(DEVICE)\n",
        "        x_lengths = torch.LongTensor([x_lengths[i] for i in sorted_indexes]).to(DEVICE)\n",
        "\n",
        "        formatted_minibatches.append([x, y, x_lengths, longest_sentence])\n",
        "\n",
        "    return formatted_minibatches\n",
        "\n",
        "  \n",
        "def create_categorical_distribution(*datasets):\n",
        "    counter = 0\n",
        "    cat_dist = {}\n",
        "\n",
        "    for dataset in datasets:\n",
        "        for sentence in dataset:\n",
        "            for word in sentence:\n",
        "                if word not in cat_dist:\n",
        "                    cat_dist[word] = 0\n",
        "\n",
        "                cat_dist[word] += 1\n",
        "                counter += 1\n",
        "\n",
        "    for key in cat_dist:\n",
        "        cat_dist[key] /= counter\n",
        "\n",
        "    return cat_dist\n",
        "  \n",
        "\n",
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "      \n",
        "def half_learning_rate(optimizer):\n",
        "    global LEARNING_RATE\n",
        "\n",
        "    LEARNING_RATE /= 2\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = LEARNING_RATE\n",
        "        \n",
        "        \n",
        "def save_model(model, name):\n",
        "    global GOOGLE_SCHOLAR\n",
        "    path = \"\"\n",
        "\n",
        "    if GOOGLE_SCHOLAR:\n",
        "        path = F\"/content/gdrive/My Drive/nlp2/{name}.pt\" \n",
        "    \n",
        "    else:\n",
        "        path = F\"./data/{name}.pt\"\n",
        "\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, name):\n",
        "    global GOOGLE_SCHOLAR\n",
        "    path = \"\"\n",
        "\n",
        "    if GOOGLE_SCHOLAR:\n",
        "        path = F\"/content/gdrive/My Drive/nlp2/{name}.pt\" \n",
        "    \n",
        "    else:\n",
        "        path = F\"./data/{name}.pt\"\n",
        "\n",
        "    model.load_state_dict(torch.load(path))\n",
        "  \n",
        "    \n",
        "    \n",
        "def train_model(model, optimizer, train_data, vocab_size):\n",
        "    global LEARNING_RATE\n",
        "    global CLIP_VALUE\n",
        "    global MINIBATCH_SIZE\n",
        "    \n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for train_batch in train_data:\n",
        "        model.zero_grad()\n",
        "\n",
        "        x, y, x_lengths, seq_len = train_batch\n",
        "        y_hat = model(x)\n",
        "        \n",
        "        #print(x.size())\n",
        "        #print(y_hat.size())\n",
        "        #print(y.size())\n",
        "        loss = model.loss(y_hat, y, MINIBATCH_SIZE, seq_len, vocab_size)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_train_loss += MINIBATCH_SIZE * loss.item()\n",
        "        \n",
        "        # not sure about good clip values, using value from word language model\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n",
        "        \n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-LEARNING_RATE, p.grad.data)\n",
        "    \n",
        "    print(F\"training loss: {total_train_loss}\")\n",
        "    \n",
        "    return total_train_loss\n",
        "    \n",
        "def evaluate_model(model, valid_data, vocab_size):\n",
        "    global MINIBATCH_SIZE\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    total_valid_loss = 0\n",
        "    total_sentences_lengths = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for valid_batch in valid_data:\n",
        "          model.zero_grad()\n",
        "\n",
        "          x, y, x_lengths, seq_len = valid_batch\n",
        "          y_hat = model(x)\n",
        "\n",
        "          batch_loss = model.loss(y_hat, y, MINIBATCH_SIZE, seq_len, vocab_size)\n",
        "          total_valid_loss += batch_loss.item()\n",
        "          \n",
        "          total_sentences_lengths += reduce(lambda a,b: a+b, x_lengths.tolist())\n",
        "    \n",
        "    total_valid_loss = total_valid_loss / total_sentences_lengths\n",
        "    \n",
        "    print(F\"validation loss: {total_valid_loss}\")\n",
        "\n",
        "    \n",
        "    ppl = 0\n",
        "    \n",
        "    try:\n",
        "      ppl = math.exp(total_valid_loss)\n",
        "    except OverflowError:\n",
        "      ppl = math.inf\n",
        "      \n",
        "    print(F\"PPL: {ppl}\")\n",
        "      \n",
        "\n",
        "    return total_valid_loss, ppl\n",
        "  \n",
        "  \n",
        "def should_keep_training(valid_loss):\n",
        "    global BEST_VALID_LOSS\n",
        "    global VALID_LOSS_DECREASED_LAST_EPOCH\n",
        "    \n",
        "    save_model = True\n",
        "    keep_training = True\n",
        "    \n",
        "    \n",
        "    if not BEST_VALID_LOSS:\n",
        "        print(\"nulth\")\n",
        "        BEST_VALID_LOSS = valid_loss\n",
        "        \n",
        "    else:\n",
        "        if valid_loss < BEST_VALID_LOSS:\n",
        "            VALID_LOSS_DECREASED_LAST_EPOCH = True\n",
        "            BEST_VALID_LOSS = valid_loss\n",
        "            \n",
        "        else:\n",
        "            save_model = False \n",
        "         \n",
        "            if VALID_LOSS_DECREASED_LAST_EPOCH:\n",
        "                print(\"first\")\n",
        "                half_learning_rate(optimizer)\n",
        "                VALID_LOSS_DECREASED_LAST_EPOCH = False\n",
        "            \n",
        "            else:\n",
        "                print(\"second\")\n",
        "                keep_training = False\n",
        "            \n",
        "    return save_model, keep_training\n",
        "  \n",
        "  \n",
        "def sample(model, vocab, ids2words, sentence_len=25):\n",
        "    global MINIBATCH_SIZE\n",
        "    global DEVICE\n",
        "    \n",
        "    predictions = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        previous_x = torch.zeros(MINIBATCH_SIZE, 1, dtype=torch.long).fill_(vocab[SOS]).to(DEVICE)\n",
        "        hidden = model.init_hidden()\n",
        "        \n",
        "        for t in range(sentence_len):\n",
        "            scores, hidden = model.step(previous_x, hidden)\n",
        "            \n",
        "            p = torch.argmax(scores, dim=-1)\n",
        "            predictions.append(p)\n",
        "            \n",
        "            prev_x = p.view(MINIBATCH_SIZE, 1)\n",
        "    \n",
        "    \n",
        "    predictions = torch.cat(predictions, dim=1).tolist()\n",
        "    \n",
        "    for i in range(10):\n",
        "        prediction = predictions[i]\n",
        "        prediction_str = ' '.join([ids2words[i] for i in prediction])\n",
        "\n",
        "        print(prediction_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLYABK5yswwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8b3prYrE2E2",
        "colab_type": "text"
      },
      "source": [
        "Initialisation of globals and constants, loading and manipulating the data, instantiating the model and training it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FZDKZry2em8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#globals and constants\n",
        "SOS = \"<SOS>\"\n",
        "EOS = \"<EOS>\"\n",
        "PAD = \"<PAD>\"\n",
        "UNKNOWN = \"<UNK>\"\n",
        "MINIBATCH_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_DIM = 256\n",
        "NUM_LSTM_UNITS = 256\n",
        "NUM_LSTM_LAYERS = 1\n",
        "DROPOUT_FACTOR = 0.75\n",
        "CLIP_VALUE = 0.25\n",
        "ON_GPU = True\n",
        "GOOGLE_SCHOLAR = True\n",
        "BEST_VALID_LOSS = False\n",
        "VALID_LOSS_DECREASED_LAST_EPOCH = True\n",
        "KEEP_TRAINING = True\n",
        "DEVICE = torch.device(\"cuda\" if ON_GPU else \"cpu\")\n",
        "ONLY_ALLOW_FREQUENT_WORDS = True\n",
        "DROP_LONG_SENTENCES = True\n",
        "USE_HALF_SETS = False\n",
        "SAMPLE = False\n",
        "\n",
        "\n",
        "#loading data and vocab as words\n",
        "#train_data, valid_data, test_data, vocab = load_data() \n",
        "train_data, valid_data, vocab, ids2words = load_data()\n",
        "len_vocab = len(vocab)\n",
        "\n",
        "train_minibatches = format_minibatches(get_minibatches(train_data), vocab)\n",
        "valid_minibatches = format_minibatches(get_minibatches(valid_data), vocab)\n",
        "\n",
        "model = DeterministicLSTM(\n",
        "    vocab,\n",
        "    NUM_LSTM_LAYERS,\n",
        "    NUM_LSTM_UNITS,\n",
        "    EMBEDDING_DIM,\n",
        "    MINIBATCH_SIZE,\n",
        "    ON_GPU,\n",
        "    PAD,\n",
        "    UNKNOWN,\n",
        "    ONLY_ALLOW_FREQUENT_WORDS,\n",
        "    DROPOUT_FACTOR\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "training_scores = []\n",
        "validation_scores = []\n",
        "ppl_scores = []\n",
        "do_keep_training = True\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(F\"epoch: {epoch}\")\n",
        "    \n",
        "    total_train_loss = train_model(model, optimizer, train_minibatches, len_vocab)\n",
        "    total_valid_loss, ppl = evaluate_model(model, valid_minibatches, len_vocab)\n",
        "    \n",
        "    training_scores.append(total_train_loss)\n",
        "    validation_scores.append(total_valid_loss)\n",
        "    ppl_scores.append(ppl)\n",
        "    \n",
        "    \n",
        "    if SAMPLE:\n",
        "        sample(model, vocab, ids2words, 10)\n",
        "    \n",
        "    do_save_model, do_keep_training = should_keep_training(total_valid_loss)\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    \n",
        "    if do_save_model:\n",
        "        save_model(model, \"model_full\")\n",
        "    \n",
        "    if not do_keep_training:\n",
        "        break\n",
        "        \n",
        "\n",
        "print(\"Training done..\")\n",
        "print(\"Training scores\")\n",
        "print(training_scores)\n",
        "print(\"Validation scores\")\n",
        "print(validation_scores)\n",
        "print(\"Ppl scores\")\n",
        "print(ppl_scores)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}