{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook3_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH7nt6PsB_jS",
        "colab_type": "text"
      },
      "source": [
        "Imports needed to run the cells below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6LifADHAtJa",
        "colab_type": "code",
        "outputId": "fc39f46b-c113-4df2-ed13-17ed44beb46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from functools import reduce\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive.mount('/content/gdrive')\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 5.0MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.3)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OAWADu_CLJ2",
        "colab_type": "text"
      },
      "source": [
        "The deterministic model implemented for the assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TaxuOgvAwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicLSTM(nn.Module):\n",
        "    def __init__(self, vocab, nb_layers=1, nb_lstm_units=100, embedding_dim=10, batch_size=64, on_gpu=False, pad_token=\"<PAD>\", unk_token=\"<UNK>\", reduced_vocab=False, dropout=0):\n",
        "        super(DeterministicLSTM, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.nb_layers = nb_layers\n",
        "        self.nb_lstm_units = nb_lstm_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.on_gpu = on_gpu\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.reduced_vocab = reduced_vocab\n",
        "        self.len_vocab = len(self.vocab) \n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.__build_model()\n",
        "        self.init_weights()\n",
        "\n",
        "    def __build_model(self):        \n",
        "        # encoder to word embeddings\n",
        "        self.encoder = nn.Embedding(\n",
        "            num_embeddings=self.len_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            padding_idx=self.vocab[self.pad_token]\n",
        "        )\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.nb_lstm_units,\n",
        "            num_layers=self.nb_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # decoder to output space\n",
        "        self.decoder = nn.Linear(self.nb_lstm_units, self.len_vocab)\n",
        "        \n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "        \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden_a = torch.zeros(self.nb_layers,\n",
        "                               self.batch_size, self.nb_lstm_units)\n",
        "        hidden_b = torch.zeros(self.nb_layers,\n",
        "                               self.batch_size, self.nb_lstm_units)\n",
        "\n",
        "        if self.on_gpu:\n",
        "            hidden_a = hidden_a.cuda()\n",
        "            hidden_b = hidden_b.cuda()\n",
        "\n",
        "        hidden_a = Variable(hidden_a)\n",
        "        hidden_b = Variable(hidden_b)\n",
        "\n",
        "        return (hidden_a, hidden_b)\n",
        "      \n",
        "    \n",
        "    def step(self, x, hidden):\n",
        "        x = self.encoder(x)\n",
        "        x = self.dropout_layer(x)\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = self.dropout_layer(x)\n",
        "        x = self.decoder(x)\n",
        "        \n",
        "        return x, hidden\n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden = self.init_hidden()\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(x.size(1)):\n",
        "            previous_x = x[:, t].unsqueeze(-1)\n",
        "            y_hat, hidden = self.step(previous_x, hidden)\n",
        "             \n",
        "            #print(y_hat.size())\n",
        "            outputs.append(y_hat)\n",
        "            \n",
        "        return torch.cat(outputs, dim=1)\n",
        "        \n",
        "    \n",
        "    def loss(self, Y_hat, Y, batch_size, seq_len, vocab_size, eval=False):\n",
        "        '''tag_pad_token = self.vocab[self.pad_token]\n",
        "        mask = (Y > tag_pad_token).float()\n",
        "        nb_tokens = int(torch.sum(mask).data)\n",
        "        \n",
        "        Y_hat = Y_hat.view(-1, nb_tokens)'''\n",
        "        \n",
        "        Y_hat = Y_hat.permute(0,2,1)\n",
        "        \n",
        "        loss = F.cross_entropy(\n",
        "            Y_hat,\n",
        "            Y, \n",
        "            ignore_index=self.vocab[self.pad_token], \n",
        "            reduction=\"none\"\n",
        "        )\n",
        "        \n",
        "        #print(loss)\n",
        "        \n",
        "        loss = loss.sum()\n",
        "        \n",
        "        \n",
        "        if eval:\n",
        "            loss = loss.sum()\n",
        "        else:\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "    '''\n",
        "\n",
        "    def loss(self, Y_hat, Y):\n",
        "        # TRICK 3 ********************************\n",
        "        # before we calculate the negative log likelihood, we need to mask out the activations\n",
        "        # this means we don't want to take into account padded items in the output vector\n",
        "        # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence\n",
        "        # and calculate the loss on that.\n",
        "\n",
        "        # flatten all the labels\n",
        "        Y = Y.view(-1)\n",
        "        print(Y)\n",
        "        # flatten all predictions\n",
        "        Y_hat = Y_hat.view(-1, self.len_vocab)\n",
        "        \n",
        "        print(Y_hat)\n",
        "        print(\"--\")\n",
        "\n",
        "        # create a mask by filtering out all tokens that ARE NOT the padding token NOR the UNK token for reduced_vocab\n",
        "        mask = None\n",
        "        \n",
        "        #if not self.reduced_vocab:\n",
        "        tag_pad_token = self.vocab[self.pad_token]\n",
        "        mask = (Y > tag_pad_token).float()\n",
        "        \n",
        "        #else:\n",
        "        #    tag_unk_token = self.vocab[self.unk_token]\n",
        "        #    mask = (Y > tag_unk_token).float()\n",
        "\n",
        "        # count how many tokens we have\n",
        "        # nb_tokens = int(torch.sum(mask).data[0])\n",
        "        nb_tokens = int(torch.sum(mask).data)\n",
        "\n",
        "        \n",
        "        \n",
        "        # pick the values for the label and zero out the rest with the mask\n",
        "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
        "\n",
        "\n",
        "        # compute cross entropy loss which ignores all <PAD> tokens\n",
        "        ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
        "\n",
        "        return ce_loss\n",
        "    '''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX4KAZALEVgZ",
        "colab_type": "text"
      },
      "source": [
        "Functions implemented to load and manipulate data, as well as wrapper functions for operations such as training, evaluating, and other things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_9jr_NITBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    global ONLY_ALLOW_FREQUENT_WORDS\n",
        "    global UNKNOWN\n",
        "    global PAD\n",
        "    global SOS\n",
        "    global EOS\n",
        "    global DROP_LONG_SENTENCES\n",
        "    global USE_HALF_SETS\n",
        "    \n",
        "    #file_ids are in the order train, valid, test\n",
        "    #file_ids = [\"1jGgW9oyEWuKgMt32_s9BnSJt3A5CR7Z8\", \"1zwVW6-HA3KxyDuJIXuTK1OvKeoXiPHqb\", \"1f-rz6KNWUPO5ToHeLewlHezDNUQdNnQg\"]\n",
        "    file_ids = [\"1tCivrO7xa9PzroVUw8s92nI7LtW6TOB5\", \"1zwVW6-HA3KxyDuJIXuTK1OvKeoXiPHqb\"]\n",
        "    \n",
        "    data = []\n",
        "    vocab = [PAD, UNKNOWN, SOS, EOS]\n",
        "    word_frequencies = {}\n",
        "\n",
        "\n",
        "    for file_id in file_ids:\n",
        "        sentences = json.loads(drive.CreateFile({'id': file_id}).GetContentString())\n",
        "        \n",
        "        if USE_HALF_SETS:\n",
        "            sentences = sentences[:len(sentences)//2]\n",
        "\n",
        "        if DROP_LONG_SENTENCES:\n",
        "            sentences = list(filter(lambda s: len(s) < 50, sentences))\n",
        "            \n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                if word not in word_frequencies:\n",
        "                    word_frequencies[word] = 0\n",
        "\n",
        "                word_frequencies[word] += 1\n",
        "\n",
        "        data.append(sentences)\n",
        "        \n",
        "    if not ONLY_ALLOW_FREQUENT_WORDS:\n",
        "        vocab += word_frequencies.keys()\n",
        "    \n",
        "    else:\n",
        "        updated_data = []\n",
        "        \n",
        "        for dataset in data:\n",
        "            new_sentences = []\n",
        "\n",
        "            for sentence in dataset:\n",
        "                new_sentences.append([w if word_frequencies[w] > 1 else UNKNOWN for w in sentence])\n",
        "\n",
        "            updated_data.append(new_sentences)\n",
        "        \n",
        "        data = updated_data\n",
        "        \n",
        "        vocab += list(filter(lambda w: word_frequencies[w] > 1, word_frequencies.keys()))\n",
        "    \n",
        "    dict_vocab = {}\n",
        "    \n",
        "    for i in range(len(vocab)):\n",
        "        dict_vocab[vocab[i]] = i\n",
        "    \n",
        "    data.append(dict_vocab)\n",
        "    data.append(vocab)\n",
        "\n",
        "    return data\n",
        "\n",
        "          \n",
        "def get_indexed_vocab(vocab):\n",
        "    indexed_vocab = {}\n",
        "    counter = 0\n",
        "\n",
        "    for w in vocab:\n",
        "        indexed_vocab[w] = counter\n",
        "        counter += 1\n",
        "\n",
        "    return indexed_vocab\n",
        "\n",
        "\n",
        "def get_longest_sentence(*datasets):\n",
        "    longest_sentence = 0\n",
        "\n",
        "    for ds in datasets:\n",
        "        candidate = len(max(ds, key=len))\n",
        "\n",
        "        if candidate > longest_sentence:\n",
        "            longest_sentence = candidate\n",
        "\n",
        "    return longest_sentence\n",
        "\n",
        "\n",
        "def get_minibatches(dataset):\n",
        "    global MINIBATCH_SIZE\n",
        "    len_d = len(dataset)\n",
        "\n",
        "    cutoff = len_d % MINIBATCH_SIZE\n",
        "    cut_dataset = dataset[: len_d - cutoff]\n",
        "\n",
        "    return [cut_dataset[i * MINIBATCH_SIZE: (i + 1) * MINIBATCH_SIZE] for i in range(len_d//MINIBATCH_SIZE)]\n",
        "\n",
        "\n",
        "def format_minibatches(minibatches, vocab):\n",
        "    global PAD\n",
        "    global SOS\n",
        "    global EOS\n",
        "    global DEVICE\n",
        "\n",
        "    pad_token = vocab[PAD]\n",
        "    formatted_minibatches = []\n",
        "\n",
        "    for minibatch in minibatches:\n",
        "        x = []\n",
        "        y = []\n",
        "        x_lengths = []\n",
        "        longest_sentence = len(max(minibatch, key=len))\n",
        "\n",
        "        for sentence in minibatch:\n",
        "            s_len = len(sentence)\n",
        "            x_lengths.append(s_len)\n",
        "            v_s = [vocab[w] for w in sentence]\n",
        "\n",
        "            ind_s_x = [vocab[SOS]] + v_s\n",
        "            pad_s_x = np.ones((longest_sentence)) * pad_token\n",
        "            pad_s_x[0:s_len] = np.array(ind_s_x[:s_len])\n",
        "\n",
        "            x.append(pad_s_x)\n",
        "\n",
        "            ind_s_y = v_s + [vocab[SOS]]\n",
        "            pad_s_y = np.ones((longest_sentence)) * pad_token\n",
        "            pad_s_y[0:s_len] = np.array(ind_s_y[:s_len])\n",
        "\n",
        "            y.append(pad_s_y)\n",
        "\n",
        "        # have to sort for later use by padded_sequence\n",
        "        sorted_indexes = sorted(range(len(x_lengths)),\n",
        "                                key=x_lengths.__getitem__)\n",
        "        sorted_indexes.reverse()\n",
        "\n",
        "        x = torch.LongTensor([x[i] for i in sorted_indexes]).to(DEVICE)\n",
        "        y = torch.LongTensor([y[i] for i in sorted_indexes]).to(DEVICE)\n",
        "        x_lengths = torch.LongTensor([x_lengths[i] for i in sorted_indexes]).to(DEVICE)\n",
        "\n",
        "        formatted_minibatches.append([x, y, x_lengths, longest_sentence])\n",
        "\n",
        "    return formatted_minibatches\n",
        "\n",
        "  \n",
        "def create_categorical_distribution(*datasets):\n",
        "    counter = 0\n",
        "    cat_dist = {}\n",
        "\n",
        "    for dataset in datasets:\n",
        "        for sentence in dataset:\n",
        "            for word in sentence:\n",
        "                if word not in cat_dist:\n",
        "                    cat_dist[word] = 0\n",
        "\n",
        "                cat_dist[word] += 1\n",
        "                counter += 1\n",
        "\n",
        "    for key in cat_dist:\n",
        "        cat_dist[key] /= counter\n",
        "\n",
        "    return cat_dist\n",
        "  \n",
        "\n",
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "      \n",
        "def half_learning_rate(optimizer):\n",
        "    global LEARNING_RATE\n",
        "\n",
        "    LEARNING_RATE /= 2\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = LEARNING_RATE\n",
        "        \n",
        "        \n",
        "def save_model(model, name):\n",
        "    global GOOGLE_SCHOLAR\n",
        "    path = \"\"\n",
        "\n",
        "    if GOOGLE_SCHOLAR:\n",
        "        path = F\"/content/gdrive/My Drive/nlp2/{name}.pt\" \n",
        "    \n",
        "    else:\n",
        "        path = F\"./data/{name}.pt\"\n",
        "\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, name):\n",
        "    global GOOGLE_SCHOLAR\n",
        "    path = \"\"\n",
        "\n",
        "    if GOOGLE_SCHOLAR:\n",
        "        path = F\"/content/gdrive/My Drive/nlp2/{name}.pt\" \n",
        "    \n",
        "    else:\n",
        "        path = F\"./data/{name}.pt\"\n",
        "\n",
        "    model.load_state_dict(torch.load(path))\n",
        "  \n",
        "    \n",
        "    \n",
        "def train_model(model, optimizer, train_data, vocab_size):\n",
        "    global LEARNING_RATE\n",
        "    global CLIP_VALUE\n",
        "    global MINIBATCH_SIZE\n",
        "    \n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for train_batch in train_data:\n",
        "        model.zero_grad()\n",
        "\n",
        "        x, y, x_lengths, seq_len = train_batch\n",
        "        y_hat = model(x)\n",
        "        \n",
        "        #print(x.size())\n",
        "        #print(y_hat.size())\n",
        "        #print(y.size())\n",
        "        loss = model.loss(y_hat, y, MINIBATCH_SIZE, seq_len, vocab_size)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_train_loss += MINIBATCH_SIZE * loss.item()\n",
        "        \n",
        "        # not sure about good clip values, using value from word language model\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n",
        "        \n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-LEARNING_RATE, p.grad.data)\n",
        "    \n",
        "    print(F\"training loss: {total_train_loss}\")\n",
        "    \n",
        "    return total_train_loss\n",
        "    \n",
        "def evaluate_model(model, valid_data, vocab_size):\n",
        "    global MINIBATCH_SIZE\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    total_valid_loss = 0\n",
        "    total_sentences_lengths = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for valid_batch in valid_data:\n",
        "          model.zero_grad()\n",
        "\n",
        "          x, y, x_lengths, seq_len = valid_batch\n",
        "          y_hat = model(x)\n",
        "\n",
        "          batch_loss = model.loss(y_hat, y, MINIBATCH_SIZE, seq_len, vocab_size)\n",
        "          total_valid_loss += batch_loss.item()\n",
        "          \n",
        "          total_sentences_lengths += reduce(lambda a,b: a+b, x_lengths.tolist())\n",
        "    \n",
        "    total_valid_loss = total_valid_loss / total_sentences_lengths\n",
        "    \n",
        "    print(F\"validation loss: {total_valid_loss}\")\n",
        "\n",
        "    \n",
        "    ppl = 0\n",
        "    \n",
        "    try:\n",
        "      ppl = math.exp(total_valid_loss)\n",
        "    except OverflowError:\n",
        "      ppl = math.inf\n",
        "      \n",
        "    print(F\"PPL: {ppl}\")\n",
        "      \n",
        "\n",
        "    return total_valid_loss, ppl\n",
        "  \n",
        "  \n",
        "def should_keep_training(valid_loss):\n",
        "    global BEST_VALID_LOSS\n",
        "    global VALID_LOSS_DECREASED_LAST_EPOCH\n",
        "    \n",
        "    save_model = True\n",
        "    keep_training = True\n",
        "    \n",
        "    \n",
        "    if not BEST_VALID_LOSS:\n",
        "        print(\"nulth\")\n",
        "        BEST_VALID_LOSS = valid_loss\n",
        "        \n",
        "    else:\n",
        "        if valid_loss < BEST_VALID_LOSS:\n",
        "            VALID_LOSS_DECREASED_LAST_EPOCH = True\n",
        "            BEST_VALID_LOSS = valid_loss\n",
        "            \n",
        "        else:\n",
        "            save_model = False \n",
        "         \n",
        "            if VALID_LOSS_DECREASED_LAST_EPOCH:\n",
        "                print(\"first\")\n",
        "                half_learning_rate(optimizer)\n",
        "                VALID_LOSS_DECREASED_LAST_EPOCH = False\n",
        "            \n",
        "            else:\n",
        "                print(\"second\")\n",
        "                keep_training = False\n",
        "            \n",
        "    return save_model, keep_training\n",
        "  \n",
        "  \n",
        "def sample(model, vocab, ids2words, sentence_len=25):\n",
        "    global MINIBATCH_SIZE\n",
        "    global DEVICE\n",
        "    \n",
        "    predictions = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        previous_x = torch.zeros(MINIBATCH_SIZE, 1, dtype=torch.long).fill_(vocab[SOS]).to(DEVICE)\n",
        "        hidden = model.init_hidden()\n",
        "        \n",
        "        for t in range(sentence_len):\n",
        "            scores, hidden = model.step(previous_x, hidden)\n",
        "            \n",
        "            p = torch.argmax(scores, dim=-1)\n",
        "            predictions.append(p)\n",
        "            \n",
        "            prev_x = p.view(MINIBATCH_SIZE, 1)\n",
        "    \n",
        "    \n",
        "    predictions = torch.cat(predictions, dim=1).tolist()\n",
        "    \n",
        "    for i in range(10):\n",
        "        prediction = predictions[i]\n",
        "        prediction_str = ' '.join([ids2words[i] for i in prediction])\n",
        "\n",
        "        print(prediction_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLYABK5yswwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8b3prYrE2E2",
        "colab_type": "text"
      },
      "source": [
        "Initialisation of globals and constants, loading and manipulating the data, instantiating the model and training it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FZDKZry2em8",
        "colab_type": "code",
        "outputId": "a3e86050-db9f-4ba3-80df-0d52884d0a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2833
        }
      },
      "source": [
        "#globals and constants\n",
        "SOS = \"<SOS>\"\n",
        "EOS = \"<EOS>\"\n",
        "PAD = \"<PAD>\"\n",
        "UNKNOWN = \"<UNK>\"\n",
        "MINIBATCH_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_DIM = 256\n",
        "NUM_LSTM_UNITS = 256\n",
        "NUM_LSTM_LAYERS = 1\n",
        "DROPOUT_FACTOR = 0.75\n",
        "CLIP_VALUE = 0.25\n",
        "ON_GPU = True\n",
        "GOOGLE_SCHOLAR = True\n",
        "BEST_VALID_LOSS = False\n",
        "VALID_LOSS_DECREASED_LAST_EPOCH = True\n",
        "KEEP_TRAINING = True\n",
        "DEVICE = torch.device(\"cuda\" if ON_GPU else \"cpu\")\n",
        "ONLY_ALLOW_FREQUENT_WORDS = True\n",
        "DROP_LONG_SENTENCES = True\n",
        "USE_HALF_SETS = False\n",
        "SAMPLE = False\n",
        "\n",
        "\n",
        "#loading data and vocab as words\n",
        "#train_data, valid_data, test_data, vocab = load_data() \n",
        "train_data, valid_data, vocab, ids2words = load_data()\n",
        "len_vocab = len(vocab)\n",
        "\n",
        "train_minibatches = format_minibatches(get_minibatches(train_data), vocab)\n",
        "valid_minibatches = format_minibatches(get_minibatches(valid_data), vocab)\n",
        "\n",
        "model = DeterministicLSTM(\n",
        "    vocab,\n",
        "    NUM_LSTM_LAYERS,\n",
        "    NUM_LSTM_UNITS,\n",
        "    EMBEDDING_DIM,\n",
        "    MINIBATCH_SIZE,\n",
        "    ON_GPU,\n",
        "    PAD,\n",
        "    UNKNOWN,\n",
        "    ONLY_ALLOW_FREQUENT_WORDS,\n",
        "    DROPOUT_FACTOR\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "training_scores = []\n",
        "validation_scores = []\n",
        "ppl_scores = []\n",
        "do_keep_training = True\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(F\"epoch: {epoch}\")\n",
        "    \n",
        "    total_train_loss = train_model(model, optimizer, train_minibatches, len_vocab)\n",
        "    total_valid_loss, ppl = evaluate_model(model, valid_minibatches, len_vocab)\n",
        "    \n",
        "    training_scores.append(total_train_loss)\n",
        "    validation_scores.append(total_valid_loss)\n",
        "    ppl_scores.append(ppl)\n",
        "    \n",
        "    \n",
        "    if SAMPLE:\n",
        "        sample(model, vocab, ids2words, 10)\n",
        "    \n",
        "    do_save_model, do_keep_training = should_keep_training(total_valid_loss)\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    \n",
        "    if do_save_model:\n",
        "        save_model(model, \"model_full\")\n",
        "    \n",
        "    if not do_keep_training:\n",
        "        break\n",
        "        \n",
        "\n",
        "print(\"Training done..\")\n",
        "print(\"Training scores\")\n",
        "print(training_scores)\n",
        "print(\"Validation scores\")\n",
        "print(validation_scores)\n",
        "print(\"Ppl scores\")\n",
        "print(ppl_scores)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "training loss: 423535989.1875\n",
            "validation loss: 6.921768787842443\n",
            "PPL: 1014.1121583454421\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "The Mr. , , , , , , , ,\n",
            "nulth\n",
            "\n",
            "\n",
            "epoch: 1\n",
            "training loss: 395124812.125\n",
            "validation loss: 6.624879571077621\n",
            "PPL: 753.6134502337645\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "The The , , , , , , , ,\n",
            "\n",
            "\n",
            "epoch: 2\n",
            "training loss: 382958098.65625\n",
            "validation loss: 6.419578914935151\n",
            "PPL: 613.744621012037\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "\n",
            "\n",
            "epoch: 3\n",
            "training loss: 374898022.09375\n",
            "validation loss: 6.290990215763962\n",
            "PPL: 539.6874716248318\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "The The Mr. said said said said said said said\n",
            "\n",
            "\n",
            "epoch: 4\n",
            "training loss: 368740572.0625\n",
            "validation loss: 6.189535711479288\n",
            "PPL: 487.61965744030203\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "The The The said said said said said said said\n",
            "\n",
            "\n",
            "epoch: 5\n",
            "training loss: 363616749.03125\n",
            "validation loss: 6.093310172069382\n",
            "PPL: 442.88501302258004\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "The But But But said said said said said said\n",
            "\n",
            "\n",
            "epoch: 6\n",
            "training loss: 359143471.65625\n",
            "validation loss: 6.0141318131906765\n",
            "PPL: 409.17044828026206\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "\n",
            "\n",
            "epoch: 7\n",
            "training loss: 355152315.21875\n",
            "validation loss: 5.939999190936838\n",
            "PPL: 379.93462214691084\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "\n",
            "\n",
            "epoch: 8\n",
            "training loss: 351597824.84375\n",
            "validation loss: 5.885357869573848\n",
            "PPL: 359.73148191596636\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "The But But But But But But But But But\n",
            "\n",
            "\n",
            "epoch: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0e13f607ef98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mF\"epoch: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_minibatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mtotal_valid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_minibatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d4ea40fa6009>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_data, vocab_size)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mMINIBATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# not sure about good clip values, using value from word language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}